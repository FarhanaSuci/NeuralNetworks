{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b42805b9-31c9-48c7-b7db-05a9e1818667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New weights found, iteration: 1 loss: 0.25617202483165047\n",
      "New weights found, iteration: 3 loss: 0.25158220991812974\n",
      "New weights found, iteration: 4 loss: 0.23235290879530385\n",
      "New weights found, iteration: 7 loss: 0.21825358151814928\n",
      "New weights found, iteration: 8 loss: 0.2070069697995982\n",
      "New weights found, iteration: 9 loss: 0.20700594759568494\n",
      "New weights found, iteration: 10 loss: 0.20159791547540779\n",
      "New weights found, iteration: 11 loss: 0.18782455240835585\n",
      "New weights found, iteration: 12 loss: 0.1857059654146867\n",
      "New weights found, iteration: 13 loss: 0.18428773122142217\n",
      "New weights found, iteration: 14 loss: 0.17756322066715868\n",
      "New weights found, iteration: 18 loss: 0.17100796337520288\n",
      "New weights found, iteration: 20 loss: 0.1668491273113996\n",
      "New weights found, iteration: 22 loss: 0.1638830224490061\n",
      "New weights found, iteration: 24 loss: 0.14817234033866988\n",
      "New weights found, iteration: 25 loss: 0.1471414576746687\n",
      "New weights found, iteration: 27 loss: 0.13968162674087456\n",
      "New weights found, iteration: 30 loss: 0.13384021035010985\n",
      "New weights found, iteration: 31 loss: 0.13193163303328304\n",
      "New weights found, iteration: 32 loss: 0.1306818261833787\n",
      "New weights found, iteration: 35 loss: 0.12388758575847467\n",
      "New weights found, iteration: 36 loss: 0.11974519135766401\n",
      "New weights found, iteration: 38 loss: 0.1184998759302214\n",
      "New weights found, iteration: 39 loss: 0.11579846202834257\n",
      "New weights found, iteration: 44 loss: 0.11346771216472376\n",
      "New weights found, iteration: 48 loss: 0.10913027780169134\n",
      "New weights found, iteration: 50 loss: 0.10711244506522202\n",
      "New weights found, iteration: 51 loss: 0.10548838879476248\n",
      "New weights found, iteration: 52 loss: 0.10035157540118313\n",
      "New weights found, iteration: 55 loss: 0.09930987547219416\n",
      "New weights found, iteration: 58 loss: 0.0973217812854136\n",
      "New weights found, iteration: 60 loss: 0.09729750847668148\n",
      "New weights found, iteration: 62 loss: 0.09609650871275328\n",
      "New weights found, iteration: 67 loss: 0.09579070563391234\n",
      "New weights found, iteration: 72 loss: 0.09451150932416795\n",
      "New weights found, iteration: 73 loss: 0.08470934503196914\n",
      "New weights found, iteration: 75 loss: 0.08264737513768824\n",
      "New weights found, iteration: 76 loss: 0.0801643783622954\n",
      "New weights found, iteration: 77 loss: 0.07940942036138754\n",
      "New weights found, iteration: 78 loss: 0.07461155589343782\n",
      "New weights found, iteration: 84 loss: 0.06962648074883769\n",
      "New weights found, iteration: 85 loss: 0.06575266382155703\n",
      "New weights found, iteration: 86 loss: 0.06291753837828006\n",
      "New weights found, iteration: 99 loss: 0.061915265763601825\n",
      "New weights found, iteration: 105 loss: 0.060227330619398\n",
      "New weights found, iteration: 111 loss: 0.058305203507737394\n",
      "New weights found, iteration: 117 loss: 0.05802974571241617\n",
      "New weights found, iteration: 119 loss: 0.055419067646068354\n",
      "New weights found, iteration: 120 loss: 0.05466520447225698\n",
      "New weights found, iteration: 121 loss: 0.05382007973871664\n",
      "New weights found, iteration: 125 loss: 0.053533036497926456\n",
      "New weights found, iteration: 127 loss: 0.05256393595153125\n",
      "New weights found, iteration: 129 loss: 0.05174691427533474\n",
      "New weights found, iteration: 130 loss: 0.04805868637068893\n",
      "[[0.03458734]\n",
      " [0.1992559 ]\n",
      " [0.27686764]\n",
      " [0.72672385]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use sigmoidal activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Creating loss function\n",
    "def loss(predicted, actual):\n",
    "    return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "#hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W1 = np.random.randn(input_size, output_size)\n",
    "b1 = np.random.randn(output_size)\n",
    "\n",
    "# Define the input for the AND gate\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Define the target output for the AND gate\n",
    "target = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# final_output\n",
    "final_output=None\n",
    "\n",
    "# Create some variables to track the best loss and the associated weights and biases\n",
    "best_loss = float('inf')\n",
    "best_W1 = None\n",
    "best_b1 = None\n",
    "\n",
    "iteration=0\n",
    "\n",
    "while(best_loss>=0.05):\n",
    "    # Forward pass\n",
    "    layer_output = np.dot(x, W1) + b1\n",
    "    output = sigmoid(np.dot(x, W1) + b1)\n",
    "\n",
    "    # save final output\n",
    "    final_output=output\n",
    "\n",
    "    # Loss calculation\n",
    "    current_loss = loss(output, target)\n",
    "\n",
    "    iteration=iteration+1\n",
    "\n",
    "    if current_loss < best_loss:\n",
    "        print('New weights found, iteration:', iteration, 'loss:', current_loss)\n",
    "        # Save current weights and biases as the best ones so far\n",
    "        best_loss = current_loss\n",
    "        best_W1 = W1.copy()\n",
    "        best_b1 = b1.copy()\n",
    "\n",
    "    else:\n",
    "        # Revert weights and biases to the previous best values\n",
    "        W1 = best_W1.copy()\n",
    "        b1 = best_b1.copy()\n",
    "     \n",
    "\n",
    "    # Update weights with small random values\n",
    "    W1 += 0.09 * np.random.randn(input_size, output_size)\n",
    "    b1 += 0.09 * np.random.randn(output_size)\n",
    " \n",
    "\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609ed0d0-1064-49c8-ba5e-0d2e237ecc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.47739291  0.13368134]\n",
      " [ 0.50032916 -1.60510581]\n",
      " [ 0.46640247 -0.35323718]]\n",
      "[[ 0.76003445 -1.2980962  -1.74882876]]\n",
      "Output:\n",
      "[[0.94751777]\n",
      " [0.99665289]\n",
      " [0.92764786]\n",
      " [0.99529326]]\n",
      "Loss: 0.6879149097954227\n"
     ]
    }
   ],
   "source": [
    "# Solve the and gate problem and then calculate the loss uisng mean square error.\n",
    "# run several times of the code and compare the error. Is the result same in every case? If so, why?\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# use sigmoidal activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W1 = np.random.randn( hidden_size,input_size)\n",
    "b1 = np.random.randn(hidden_size)\n",
    "W2 = np.random.randn(output_size, hidden_size)\n",
    "b2 = np.random.randn(output_size)\n",
    "\n",
    "print(W1)\n",
    "print(W2)\n",
    "\n",
    "# Define the input for the AND gate\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Define the target output for the AND gate\n",
    "target = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Forward pass\n",
    "hidden_layer_output = (np.dot(x, np.array(W1).T) + b1)\n",
    "output = sigmoid(np.dot(hidden_layer_output, np.array(W2).T )+ b2)\n",
    "\n",
    "# Calculate the loss (using Mean Squared Error)\n",
    "loss = np.mean((output - target) ** 2)\n",
    "\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dccf5ba-97c9-4f06-a492-e662f78d9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss is:inf\n",
      "New weights found, iteration: 0 loss: 0.5424947577588415\n",
      "New weights found, iteration: 2 loss: 0.5407614086890614\n",
      "New weights found, iteration: 4 loss: 0.5201242900805575\n",
      "New weights found, iteration: 6 loss: 0.49066043574247903\n",
      "New weights found, iteration: 9 loss: 0.47286484247241073\n",
      "New weights found, iteration: 10 loss: 0.4503378066710195\n",
      "New weights found, iteration: 11 loss: 0.4381496935833472\n",
      "New weights found, iteration: 12 loss: 0.4219054821168376\n",
      "New weights found, iteration: 16 loss: 0.3912891549865602\n",
      "New weights found, iteration: 18 loss: 0.34873381372260126\n",
      "New weights found, iteration: 19 loss: 0.3285630843517353\n",
      "New weights found, iteration: 20 loss: 0.31852089061888705\n",
      "New weights found, iteration: 21 loss: 0.31347790769656175\n",
      "New weights found, iteration: 22 loss: 0.3014129451671213\n",
      "New weights found, iteration: 24 loss: 0.2839478785152941\n",
      "New weights found, iteration: 26 loss: 0.23946005179394353\n",
      "New weights found, iteration: 27 loss: 0.23695494206768172\n",
      "New weights found, iteration: 30 loss: 0.20046622751257473\n",
      "New weights found, iteration: 32 loss: 0.20045984442506068\n",
      "New weights found, iteration: 33 loss: 0.18294556713980994\n",
      "New weights found, iteration: 34 loss: 0.1429136080454344\n",
      "New weights found, iteration: 36 loss: 0.11780938552501036\n",
      "New weights found, iteration: 39 loss: 0.10851135167329903\n",
      "New weights found, iteration: 44 loss: 0.10792617689558434\n",
      "New weights found, iteration: 47 loss: 0.10368967597337847\n",
      "New weights found, iteration: 48 loss: 0.09132922536242799\n",
      "New weights found, iteration: 63 loss: 0.08055753967759419\n",
      "New weights found, iteration: 66 loss: 0.07362375405000258\n",
      "New weights found, iteration: 69 loss: 0.06962979601932223\n",
      "New weights found, iteration: 72 loss: 0.06652053859793955\n",
      "New weights found, iteration: 73 loss: 0.057127081572911555\n",
      "New weights found, iteration: 82 loss: 0.05293063605399574\n",
      "New weights found, iteration: 83 loss: 0.04671609371717481\n",
      "New weights found, iteration: 90 loss: 0.04413525633849233\n",
      "New weights found, iteration: 92 loss: 0.03628825998495581\n",
      "New weights found, iteration: 96 loss: 0.030262230151434574\n",
      "New weights found, iteration: 126 loss: 0.02527838540198135\n",
      "New weights found, iteration: 129 loss: 0.025261622898732326\n",
      "New weights found, iteration: 131 loss: 0.02359522283854478\n",
      "New weights found, iteration: 134 loss: 0.022960009671856338\n",
      "New weights found, iteration: 135 loss: 0.019582443068690742\n",
      "New weights found, iteration: 136 loss: 0.01756371352618103\n",
      "New weights found, iteration: 141 loss: 0.015589225767958487\n",
      "New weights found, iteration: 143 loss: 0.011844956569314852\n",
      "New weights found, iteration: 146 loss: 0.00857502631781595\n",
      "New weights found, iteration: 148 loss: 0.006313311651363205\n",
      "[[0.00108503]\n",
      " [0.11859426]\n",
      " [0.07565161]\n",
      " [0.91021815]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use sigmoidal activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Creating loss function\n",
    "def loss(predicted, actual):\n",
    "    return np.mean((predicted - actual) ** 2)\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.random.randn(hidden_size)\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.random.randn(output_size)\n",
    "\n",
    "# Define the input for the AND gate\n",
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Define the target output for the AND gate\n",
    "target = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# final_output\n",
    "final_output=None\n",
    "\n",
    "# Create some variables to track the best loss and the associated weights and biases\n",
    "best_loss = float('inf')\n",
    "print(f\"best loss is:{best_loss}\")\n",
    "best_W1 = None\n",
    "best_b1 = None\n",
    "best_W2 = None\n",
    "best_b2 = None\n",
    "\n",
    "for iteration in range(150):\n",
    "    # Forward pass\n",
    "    hidden_layer_output = np.dot(x, W1) + b1\n",
    "    output = sigmoid(np.dot(hidden_layer_output, W2) + b2)\n",
    "\n",
    "    # save final output\n",
    "    final_output=output\n",
    "\n",
    "    # Loss calculation\n",
    "    current_loss = loss(output, target)\n",
    "\n",
    "    if current_loss < best_loss:\n",
    "        print('New weights found, iteration:', iteration, 'loss:', current_loss)\n",
    "        # Save current weights and biases as the best ones so far\n",
    "        best_loss = current_loss\n",
    "        best_W1 = W1.copy()\n",
    "        best_b1 = b1.copy()\n",
    "        best_W2 = W2.copy()\n",
    "        best_b2 = b2.copy()\n",
    "    else:\n",
    "        # Revert weights and biases to the previous best values\n",
    "        W1 = best_W1.copy()\n",
    "        b1 = best_b1.copy()\n",
    "        W2 = best_W2.copy()\n",
    "        b2 = best_b2.copy()\n",
    "\n",
    "    # Update weights with small random values\n",
    "    W1 += 0.09 * np.random.randn(input_size, hidden_size)\n",
    "    b1 += 0.09 * np.random.randn(hidden_size)\n",
    "    W2 += 0.09 * np.random.randn(hidden_size, output_size)\n",
    "    b2 += 0.09 * np.random.randn(output_size)\n",
    "\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e54fc0e-1fc0-489c-a406-65638bdfe23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 0.7009254593641837\n",
      "Epoch 200: Loss = 0.7303958074967896\n",
      "Epoch 300: Loss = 0.7378318044761677\n",
      "Epoch 400: Loss = 0.7411948536291323\n",
      "Epoch 500: Loss = 0.7431073338026416\n",
      "Predicted Output:\n",
      "[[0.99675121]\n",
      " [0.00863753]\n",
      " [0.99487176]\n",
      " [0.00547905]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#loss=1/2*(y'-y)^2\n",
    "#loss_derivative=y'-y\n",
    "#sigmoid_derivative=y'*(1-y')\n",
    "#backpropagation=(y'-y)*y'*(1-y')\n",
    "#back_propagation=loss_derivative*sigmoid_derivative\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define the input data and labels for the AND gate problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Initialize the weights and biases with random values\n",
    "#np.random.seed(42)\n",
    "W = np.random.randn(2, 1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.8\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    z = np.dot(X, W) + b\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = np.mean((y - a) ** 2)\n",
    "\n",
    "    error= (y - a)\n",
    "    # Backpropagation\n",
    "    delta = error * sigmoid_derivative(z)\n",
    "    dW = np.dot(X.T, delta)\n",
    "    db = np.sum(delta)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
    "\n",
    "# Test the model on the input data\n",
    "#predictions = (a >= 0.5).astype(int)\n",
    "print(\"Predicted Output:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ead7c51-c9ef-42b0-b0d8-9e975875678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 0.743998459915124\n",
      "Epoch 200: Loss = 0.7470214794177472\n",
      "Epoch 300: Loss = 0.7480165507492393\n",
      "Epoch 400: Loss = 0.748512245347843\n",
      "Epoch 500: Loss = 0.7488092427954165\n",
      "Predicted Output:\n",
      "[[0.99836331]\n",
      " [0.99958641]\n",
      " [0.99966728]\n",
      " [0.999916  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#AND operation\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define the input data and labels for the AND gate problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Initialize the weights and biases with random values\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(2, 1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.8\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    z = np.dot(X, W) + b\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = np.mean((y - a) ** 2)\n",
    "\n",
    "    error= (y - a)\n",
    "    # Backpropagation\n",
    "    delta = error * sigmoid_derivative(z)\n",
    "    dW = np.dot(X.T, delta)\n",
    "    db = np.sum(delta)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
    "\n",
    "# Test the model on the input data\n",
    "#predictions = (a >= 0.5).astype(int)\n",
    "print(\"Predicted Output:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af51d409-68cf-470a-9111-edd46d3ea28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 0.7457842503821006\n",
      "Epoch 200: Loss = 0.7482270406384637\n",
      "Epoch 300: Loss = 0.7494896574546598\n",
      "Epoch 400: Loss = 0.7547234452105485\n",
      "Epoch 500: Loss = 0.9260643111014422\n",
      "Predicted Output:\n",
      "[[9.09171186e-01]\n",
      " [3.07676177e-02]\n",
      " [3.12594655e-02]\n",
      " [1.02323072e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#OR operation\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define the input data and labels for the AND gate problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# Initialize the weights and biases with random values\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(2, 1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.8\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    z = np.dot(X, W) + b\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = np.mean((y - a) ** 2)\n",
    "\n",
    "    error= (y - a)\n",
    "    # Backpropagation\n",
    "    delta = error * sigmoid_derivative(z)\n",
    "    dW = np.dot(X.T, delta)\n",
    "    db = np.sum(delta)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
    "\n",
    "# Test the model on the input data\n",
    "#predictions = (a >= 0.5).astype(int)\n",
    "print(\"Predicted Output:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45cccae6-5015-422b-83bd-3fd1fbc8bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 0.7457842503821006\n",
      "Epoch 200: Loss = 0.7482270406384637\n",
      "Epoch 300: Loss = 0.7494896574546598\n",
      "Epoch 400: Loss = 0.7547234452105485\n",
      "Epoch 500: Loss = 0.9260643111014422\n",
      "Predicted Output:\n",
      "[[9.09171186e-01]\n",
      " [3.07676177e-02]\n",
      " [3.12594655e-02]\n",
      " [1.02323072e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#NOR operation\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define the input data and labels for the AND gate problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# Initialize the weights and biases with random values\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(2, 1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.8\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    z = np.dot(X, W) + b\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = np.mean((y - a) ** 2)\n",
    "\n",
    "    error= (y - a)\n",
    "    # Backpropagation\n",
    "    delta = error * sigmoid_derivative(z)\n",
    "    dW = np.dot(X.T, delta)\n",
    "    db = np.sum(delta)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
    "\n",
    "# Test the model on the input data\n",
    "#predictions = (a >= 0.5).astype(int)\n",
    "print(\"Predicted Output:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75b8c4b5-91cd-42c0-b0a6-3da9d8622817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss = 0.7432836555297341\n",
      "Epoch 200: Loss = 0.7470809107772447\n",
      "Epoch 300: Loss = 0.7481218311762403\n",
      "Epoch 400: Loss = 0.7486111980890843\n",
      "Epoch 500: Loss = 0.7488963664946978\n",
      "Predicted Output:\n",
      "[[0.00097506]\n",
      " [0.00051197]\n",
      " [0.00072118]\n",
      " [0.00037862]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#AND operation\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define the input data and labels for the AND gate problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[1], [1], [1], [0]])\n",
    "\n",
    "# Initialize the weights and biases with random values\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(2, 1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "# Set the learning rate and number of epochs\n",
    "learning_rate = 0.8\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    z = np.dot(X, W) + b\n",
    "    a = sigmoid(z)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = np.mean((y - a) ** 2)\n",
    "\n",
    "    error= (y - a)\n",
    "    # Backpropagation\n",
    "    delta = error * sigmoid_derivative(z)\n",
    "    dW = np.dot(X.T, delta)\n",
    "    db = np.sum(delta)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    # Print the loss at every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
    "\n",
    "# Test the model on the input data\n",
    "#predictions = (a >= 0.5).astype(int)\n",
    "print(\"Predicted Output:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb428ab5-3367-41a5-9b45-9ee93945670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:0.2709390289950311\n",
      "Epoch 4000, Loss:0.016653247525513573\n",
      "Epoch 8000, Loss:0.0030754596275636937\n",
      "Predictions after training:\n",
      "[[0.03896154]\n",
      " [0.95268248]\n",
      " [0.95257076]\n",
      " [0.0491688 ]]\n"
     ]
    }
   ],
   "source": [
    "#For X-OR\n",
    "#Description:https://www.geeksforgeeks.org/backpropagation-in-neural-network/\n",
    "#Two layer concept:https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class NeuralNetwork: \n",
    "\tdef __init__(self, input_size, hidden_size, output_size): \n",
    "\t\tself.input_size = input_size \n",
    "\t\tself.hidden_size = hidden_size \n",
    "\t\tself.output_size = output_size \n",
    "\n",
    "\t\t# Initialize weights \n",
    "\t\tself.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size) \n",
    "\t\tself.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size) \n",
    "\n",
    "\t\t# Initialize the biases \n",
    "\t\tself.bias_hidden = np.zeros((1, self.hidden_size)) \n",
    "\t\tself.bias_output = np.zeros((1, self.output_size)) \n",
    "\n",
    "\tdef sigmoid(self, x): \n",
    "\t\treturn 1 / (1 + np.exp(-x)) \n",
    "\n",
    "\tdef sigmoid_derivative(self, x): \n",
    "\t\treturn x * (1 - x) \n",
    "\n",
    "\tdef feedforward(self, X): \n",
    "\t\t# Input to hidden \n",
    "\t\tself.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden \n",
    "\t\tself.hidden_output = self.sigmoid(self.hidden_activation) \n",
    "\n",
    "\t\t# Hidden to output \n",
    "\t\tself.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output \n",
    "\t\tself.predicted_output = self.sigmoid(self.output_activation) \n",
    "\n",
    "\t\treturn self.predicted_output \n",
    "\n",
    "\tdef backward(self, X, y, learning_rate): \n",
    "\t\t# Compute the output layer error \n",
    "\t\toutput_error = y - self.predicted_output \n",
    "\t\toutput_delta = output_error * self.sigmoid_derivative(self.predicted_output) \n",
    "\n",
    "\t\t# Compute the hidden layer error \n",
    "\t\thidden_error = np.dot(output_delta, self.weights_hidden_output.T) \n",
    "\t\thidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output) \n",
    "\n",
    "\t\t# Update weights and biases \n",
    "\t\tself.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate \n",
    "\t\tself.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate \n",
    "\t\tself.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate \n",
    "\t\tself.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate \n",
    "\n",
    "\tdef train(self, X, y, epochs, learning_rate): \n",
    "\t\tfor epoch in range(epochs): \n",
    "\t\t\toutput = self.feedforward(X) \n",
    "\t\t\tself.backward(X, y, learning_rate) \n",
    "\t\t\tif epoch % 4000 == 0: \n",
    "\t\t\t\tloss = np.mean(np.square(y - output)) \n",
    "\t\t\t\tprint(f\"Epoch {epoch}, Loss:{loss}\") \n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y = np.array([[0], [1], [1], [0]]) \n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1) \n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1) \n",
    "\n",
    "# Test the trained model \n",
    "output = nn.feedforward(X) \n",
    "print(\"Predictions after training:\") \n",
    "print(output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "989b15ea-8408-4ffa-a84a-037149c691f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:0.27040714284532197\n",
      "Epoch 4000, Loss:0.007396585537436983\n",
      "Epoch 8000, Loss:0.0034001372076937933\n",
      "Predictions after training:\n",
      "[[2.47775411e-04]\n",
      " [5.57563574e-02]\n",
      " [5.57563583e-02]\n",
      " [9.33637397e-01]]\n"
     ]
    }
   ],
   "source": [
    "#For AND gate\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class NeuralNetwork: \n",
    "\tdef __init__(self, input_size, output_size): \n",
    "\t\tself.input_size = input_size  \n",
    "\t\tself.output_size = output_size \n",
    "\n",
    "\t\t# Initialize weights \n",
    "\t\tself.weights_input = np.random.randn(self.input_size, self.output_size) \n",
    "\t\t# Initialize the biases \n",
    "\t\tself.bias_output = np.zeros((1, self.output_size)) \n",
    "\n",
    "\tdef sigmoid(self, x): \n",
    "\t\treturn 1 / (1 + np.exp(-x)) \n",
    "\n",
    "\tdef sigmoid_derivative(self, x): \n",
    "\t\treturn x * (1 - x) \n",
    "\n",
    "\tdef feedforward(self, X): \n",
    "\t\t \n",
    "\t\tself.output_activation = np.dot(X, self.weights_input) + self.bias_output\n",
    "\t\tself.predicted_output = self.sigmoid(self.output_activation) \n",
    "\n",
    "\t\treturn self.predicted_output \n",
    "\n",
    "\tdef backward(self, X, y, learning_rate): \n",
    "\t\t# Compute the output layer error \n",
    "\t\toutput_error = y - self.predicted_output \n",
    "\t\toutput_delta = output_error * self.sigmoid_derivative(self.predicted_output) \n",
    "\n",
    "\t\t# Update weights and biases \n",
    "\t\tself.weights_input += np.dot(X.T, output_delta) * learning_rate \n",
    "\t\tself.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate \n",
    "\n",
    "\tdef train(self, X, y, epochs, learning_rate): \n",
    "\t\tfor epoch in range(epochs): \n",
    "\t\t\toutput = self.feedforward(X) \n",
    "\t\t\tself.backward(X, y, learning_rate) \n",
    "\t\t\tif epoch % 4000 == 0: \n",
    "\t\t\t\tloss = np.mean(np.square(y - output)) \n",
    "\t\t\t\tprint(f\"Epoch {epoch}, Loss:{loss}\") \n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y = np.array([[0], [0], [0], [1]]) \n",
    "\n",
    "nn = NeuralNetwork(input_size=2, output_size=1) \n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1) \n",
    "\n",
    "# Test the trained model \n",
    "output = nn.feedforward(X) \n",
    "print(\"Predictions after training:\") \n",
    "print(output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5abfe826-de24-45ba-b38d-c706a844d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:0.2709530110636155\n",
      "Epoch 4000, Loss:0.0037477644806980996\n",
      "Epoch 8000, Loss:0.0017253244735124774\n",
      "Predictions after training:\n",
      "[[0.05508825]\n",
      " [0.96559552]\n",
      " [0.96559439]\n",
      " [0.99992599]]\n"
     ]
    }
   ],
   "source": [
    "#For OR gate\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class NeuralNetwork: \n",
    "\tdef __init__(self, input_size, output_size): \n",
    "\t\tself.input_size = input_size  \n",
    "\t\tself.output_size = output_size \n",
    "\n",
    "\t\t# Initialize weights \n",
    "\t\tself.weights_input = np.random.randn(self.input_size, self.output_size) \n",
    "\t\t# Initialize the biases \n",
    "\t\tself.bias_output = np.zeros((1, self.output_size)) \n",
    "\n",
    "\tdef sigmoid(self, x): \n",
    "\t\treturn 1 / (1 + np.exp(-x)) \n",
    "\n",
    "\tdef sigmoid_derivative(self, x): \n",
    "\t\treturn x * (1 - x) \n",
    "\n",
    "\tdef feedforward(self, X): \n",
    "\t\t \n",
    "\t\tself.output_activation = np.dot(X, self.weights_input) + self.bias_output\n",
    "\t\tself.predicted_output = self.sigmoid(self.output_activation) \n",
    "\n",
    "\t\treturn self.predicted_output \n",
    "\n",
    "\tdef backward(self, X, y, learning_rate): \n",
    "\t\t# Compute the output layer error \n",
    "\t\toutput_error = y - self.predicted_output \n",
    "\t\toutput_delta = output_error * self.sigmoid_derivative(self.predicted_output) \n",
    "\n",
    "\t\t# Update weights and biases \n",
    "\t\tself.weights_input += np.dot(X.T, output_delta) * learning_rate \n",
    "\t\tself.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate \n",
    "\n",
    "\tdef train(self, X, y, epochs, learning_rate): \n",
    "\t\tfor epoch in range(epochs): \n",
    "\t\t\toutput = self.feedforward(X) \n",
    "\t\t\tself.backward(X, y, learning_rate) \n",
    "\t\t\tif epoch % 4000 == 0: \n",
    "\t\t\t\tloss = np.mean(np.square(y - output)) \n",
    "\t\t\t\tprint(f\"Epoch {epoch}, Loss:{loss}\") \n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y = np.array([[0], [1], [1], [1]]) \n",
    "\n",
    "nn = NeuralNetwork(input_size=2, output_size=1) \n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1) \n",
    "\n",
    "# Test the trained model \n",
    "output = nn.feedforward(X) \n",
    "print(\"Predictions after training:\") \n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56b9ef78-2f7d-4914-9dff-fc8e1c92cd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:0.3288112872702391\n",
      "Epoch 4000, Loss:0.007186952528413257\n",
      "Epoch 8000, Loss:0.003351964747114018\n",
      "Predictions after training:\n",
      "[[0.99975666]\n",
      " [0.94455827]\n",
      " [0.94455827]\n",
      " [0.06598569]]\n"
     ]
    }
   ],
   "source": [
    "#For NAND gate\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class NeuralNetwork: \n",
    "\tdef __init__(self, input_size, output_size): \n",
    "\t\tself.input_size = input_size  \n",
    "\t\tself.output_size = output_size \n",
    "\n",
    "\t\t# Initialize weights \n",
    "\t\tself.weights_input = np.random.randn(self.input_size, self.output_size) \n",
    "\t\t# Initialize the biases \n",
    "\t\tself.bias_output = np.zeros((1, self.output_size)) \n",
    "\n",
    "\tdef sigmoid(self, x): \n",
    "\t\treturn 1 / (1 + np.exp(-x)) \n",
    "\n",
    "\tdef sigmoid_derivative(self, x): \n",
    "\t\treturn x * (1 - x) \n",
    "\n",
    "\tdef feedforward(self, X): \n",
    "\t\t \n",
    "\t\tself.output_activation = np.dot(X, self.weights_input) + self.bias_output\n",
    "\t\tself.predicted_output = self.sigmoid(self.output_activation) \n",
    "\n",
    "\t\treturn self.predicted_output \n",
    "\n",
    "\tdef backward(self, X, y, learning_rate): \n",
    "\t\t# Compute the output layer error \n",
    "\t\toutput_error = y - self.predicted_output \n",
    "\t\toutput_delta = output_error * self.sigmoid_derivative(self.predicted_output) \n",
    "\n",
    "\t\t# Update weights and biases \n",
    "\t\tself.weights_input += np.dot(X.T, output_delta) * learning_rate \n",
    "\t\tself.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate \n",
    "\n",
    "\tdef train(self, X, y, epochs, learning_rate): \n",
    "\t\tfor epoch in range(epochs): \n",
    "\t\t\toutput = self.feedforward(X) \n",
    "\t\t\tself.backward(X, y, learning_rate) \n",
    "\t\t\tif epoch % 4000 == 0: \n",
    "\t\t\t\tloss = np.mean(np.square(y - output)) \n",
    "\t\t\t\tprint(f\"Epoch {epoch}, Loss:{loss}\") \n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y = np.array([[1], [1], [1], [0]]) \n",
    "\n",
    "nn = NeuralNetwork(input_size=2, output_size=1) \n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1) \n",
    "\n",
    "# Test the trained model \n",
    "output = nn.feedforward(X) \n",
    "print(\"Predictions after training:\") \n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c992e1eb-1b73-4612-a3a0-4c4de8ac225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:0.16360239899675677\n",
      "Epoch 4000, Loss:0.003679168871372489\n",
      "Epoch 8000, Loss:0.001709857447719372\n",
      "Predictions after training:\n",
      "[[9.45109364e-01]\n",
      " [3.42796456e-02]\n",
      " [3.42869408e-02]\n",
      " [7.31897028e-05]]\n"
     ]
    }
   ],
   "source": [
    "#For NOR gate\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class NeuralNetwork: \n",
    "\tdef __init__(self, input_size, output_size): \n",
    "\t\tself.input_size = input_size  \n",
    "\t\tself.output_size = output_size \n",
    "\n",
    "\t\t# Initialize weights \n",
    "\t\tself.weights_input = np.random.randn(self.input_size, self.output_size) \n",
    "\t\t# Initialize the biases \n",
    "\t\tself.bias_output = np.zeros((1, self.output_size)) \n",
    "\n",
    "\tdef sigmoid(self, x): \n",
    "\t\treturn 1 / (1 + np.exp(-x)) \n",
    "\n",
    "\tdef sigmoid_derivative(self, x): \n",
    "\t\treturn x * (1 - x) \n",
    "\n",
    "\tdef feedforward(self, X): \n",
    "\t\t \n",
    "\t\tself.output_activation = np.dot(X, self.weights_input) + self.bias_output\n",
    "\t\tself.predicted_output = self.sigmoid(self.output_activation) \n",
    "\n",
    "\t\treturn self.predicted_output \n",
    "\n",
    "\tdef backward(self, X, y, learning_rate): \n",
    "\t\t# Compute the output layer error \n",
    "\t\toutput_error = y - self.predicted_output \n",
    "\t\toutput_delta = output_error * self.sigmoid_derivative(self.predicted_output) \n",
    "\n",
    "\t\t# Update weights and biases \n",
    "\t\tself.weights_input += np.dot(X.T, output_delta) * learning_rate \n",
    "\t\tself.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate \n",
    "\n",
    "\tdef train(self, X, y, epochs, learning_rate): \n",
    "\t\tfor epoch in range(epochs): \n",
    "\t\t\toutput = self.feedforward(X) \n",
    "\t\t\tself.backward(X, y, learning_rate) \n",
    "\t\t\tif epoch % 4000 == 0: \n",
    "\t\t\t\tloss = np.mean(np.square(y - output)) \n",
    "\t\t\t\tprint(f\"Epoch {epoch}, Loss:{loss}\") \n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y = np.array([[1], [0], [0], [0]]) \n",
    "\n",
    "nn = NeuralNetwork(input_size=2, output_size=1) \n",
    "nn.train(X, y, epochs=10000, learning_rate=0.1) \n",
    "\n",
    "# Test the trained model \n",
    "output = nn.feedforward(X) \n",
    "print(\"Predictions after training:\") \n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5afd91a-2156-4b65-a996-63f36811e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial hidden weights: [0.06505159 0.94888554] [0.96563203 0.80839735]\n",
      "Initial hidden biases: [0.30461377 0.09767211]\n",
      "Initial output weights: [0.68423303] [0.44015249]\n",
      "Initial output biases: [0.12203823]\n",
      "Final hidden weights: [3.75165818 5.81708984] [3.76618077 5.8897441 ]\n",
      "Final hidden bias: [-5.75514663 -2.44767828]\n",
      "Final output weights: [-8.21922452] [7.58395441]\n",
      "Final output bias: [-3.42278259]\n",
      "\n",
      "Output from neural network after 10,000 epochs: [0.05494614] [0.94962653] [0.94940782] [0.05442674]\n"
     ]
    }
   ],
   "source": [
    "#X-OR\n",
    "import numpy as np \n",
    "#np.random.seed(0)\n",
    "\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Input datasets\n",
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "expected_output = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.1\n",
    "inputLayerNeurons, hiddenLayerNeurons, outputLayerNeurons = 2,2,1\n",
    "\n",
    "#Random weights and bias initialization\n",
    "hidden_weights = np.random.uniform(size=(inputLayerNeurons,hiddenLayerNeurons))\n",
    "hidden_bias =np.random.uniform(size=(1,hiddenLayerNeurons))\n",
    "output_weights = np.random.uniform(size=(hiddenLayerNeurons,outputLayerNeurons))\n",
    "output_bias = np.random.uniform(size=(1,outputLayerNeurons))\n",
    "\n",
    "print(\"Initial hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Initial hidden biases: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Initial output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Initial output biases: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "\n",
    "#Training algorithm\n",
    "for _ in range(epochs):\n",
    "\t#Forward Propagation\n",
    "\thidden_layer_activation = np.dot(inputs,hidden_weights)\n",
    "\thidden_layer_activation += hidden_bias\n",
    "\thidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "\toutput_layer_activation = np.dot(hidden_layer_output,output_weights)\n",
    "\toutput_layer_activation += output_bias\n",
    "\tpredicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "\t#Backpropagation\n",
    "\terror = expected_output - predicted_output\n",
    "\td_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\t\n",
    "\terror_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "\td_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "\t#Updating Weights and Biases\n",
    "\toutput_weights += hidden_layer_output.T.dot(d_predicted_output) * lr\n",
    "\toutput_bias += np.sum(d_predicted_output,axis=0,keepdims=True) * lr\n",
    "\thidden_weights += inputs.T.dot(d_hidden_layer) * lr\n",
    "\thidden_bias += np.sum(d_hidden_layer,axis=0,keepdims=True) * lr\n",
    "\n",
    "print(\"Final hidden weights: \",end='')\n",
    "print(*hidden_weights)\n",
    "print(\"Final hidden bias: \",end='')\n",
    "print(*hidden_bias)\n",
    "print(\"Final output weights: \",end='')\n",
    "print(*output_weights)\n",
    "print(\"Final output bias: \",end='')\n",
    "print(*output_bias)\n",
    "\n",
    "print(\"\\nOutput from neural network after 10,000 epochs: \",end='')\n",
    "print(*predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bd8a0-9a49-4b0a-8505-72947c537150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
